import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import os

from sklearn import metrics

from layer import *
from utils import *

def CREATE_enhancer_train(
        clf, 
        train_loader, 
        valid_loader, 
        test_loader, 
        valid_labels, 
        test_labels, 
        lr=5e-5, 
        max_epoch=300, 
        pre_epoch=50, 
        seq_loss_weight=1.0,
        outdir='./output/', 
        device='cuda'
    ):
    """
    å¢å¼ºå­é¢„æµ‹æ¨¡å‹è®­ç»ƒ
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(outdir, exist_ok=True)
    os.makedirs(os.path.join(outdir, 'checkpoint'), exist_ok=True)
    
    clf = clf.to(device)
    
    # è½¬æ¢æ ‡ç­¾æ ¼å¼
    valid_labels = np.array(valid_labels).astype(int)
    test_labels = np.array(test_labels).astype(int)
    
    optimizer = optim.Adam(clf.parameters(), lr=lr)
    scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, min_lr=1e-6)
    scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=20, min_lr=1e-8)
    
    # æŸå¤±å‡½æ•°
    clf_loss_fn = nn.BCEWithLogitsLoss()
    recon_loss_fn = nn.MSELoss()
    regloss = Regularization(clf, weight_decay1=1e-8, weight_decay2=5e-7)

    loss1_rate = [0.0] * pre_epoch + [1.0] * (max_epoch - pre_epoch)
    loss2_rate = [1.0] * pre_epoch + [0.5] * (max_epoch - pre_epoch)
    
    # è®°å½•æœ€ä½³æ€§èƒ½
    max_auc = 0.0
    max_auc_epoch = 0
    
    # è®­ç»ƒè®°å½•
    train_history = {'loss': [], 'auc': []}
    val_history = {'loss': [], 'auc': []}
    
    with tqdm(range(max_epoch), total=max_epoch, desc='Epochs') as tq:
        for epoch in tq:
            # è®­ç»ƒè¾“å‡ºæ”¶é›†
            train_out = []
            train_labels_list = []
            training_loss, training_clf, training_recon = [], [], []
            
            # ========== è®­ç»ƒé˜¶æ®µ ==========
            clf.train()
            
            for seq, labels in train_loader:
                # æ”¶é›†æ ‡ç­¾
                train_labels_list.extend(labels.squeeze().cpu().numpy())
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                seq = seq.to(device)
                labels = labels.squeeze().to(device)
                
                # æ¨¡å‹å‰å‘ä¼ æ’­
                outputs = clf(seq)
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                train_out.extend(torch.sigmoid(outputs['enhancer_score'].squeeze()).cpu().detach().numpy())

                # æ­£åˆ™åŒ–æŸå¤±
                reg_loss = regloss(clf)
                
                # åˆ†ç±»æŸå¤±
                clf_loss = clf_loss_fn(outputs['enhancer_score'].squeeze(), labels.float()) + reg_loss
                
                # é‡æ„æŸå¤±
                recon_loss = seq_loss_weight * recon_loss_fn(outputs['seq_recon'], seq)
                
                # æ€»æŸå¤±
                total_loss = loss1_rate[epoch] * clf_loss + loss2_rate[epoch] * recon_loss
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()
                
                # è®°å½•æŸå¤±
                training_loss.append(total_loss.item())
                training_clf.append(clf_loss.item())
                training_recon.append(recon_loss.item())
                    
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            train_loss = float(np.mean(training_loss))
            train_recon = float(np.mean(training_recon))
            train_clf = float(np.mean(training_clf))
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            train_scores = np.array(train_out)
            train_labels_np = np.array(train_labels_list).astype(int)
            
            # ========== éªŒè¯é˜¶æ®µ ==========
            clf.eval()
            val_out = []
            val_loss_list = []
            
            with torch.no_grad():
                for seq, labels in valid_loader:
                    seq = seq.to(device)
                    labels = labels.squeeze().to(device)
                    
                    outputs = clf(seq)
                    
                    val_out.extend(torch.sigmoid(outputs['enhancer_score'].squeeze()).cpu().numpy())
                    
                    val_clf_loss = clf_loss_fn(outputs['enhancer_score'].squeeze(), labels.float())
                    val_recon_loss = recon_loss_fn(outputs['seq_recon'], seq)
                    val_total_loss = val_clf_loss + val_recon_loss
                    val_loss_list.append(val_total_loss.item())
                            
            val_scores = np.array(val_out)
            val_loss = np.mean(val_loss_list)
            
            # ========== è®¡ç®—è¯„ä¼°æŒ‡æ ‡ ==========
            if epoch >= pre_epoch:
                # è®¡ç®—AUC
                train_auc = metrics.roc_auc_score(train_labels_np, train_scores)
                val_auc = metrics.roc_auc_score(valid_labels, val_scores)
                
                print(f"\nEpoch {epoch+1}/{max_epoch}")
                print(f"Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}")
            else:
                val_auc = 0.0
            
            # è¿›åº¦æ¡ä¿¡æ¯
            if epoch < pre_epoch:
                epoch_info = f'recon={train_recon:.3f}'
            else:
                epoch_info = f'recon={train_recon:.3f}, clf={train_clf:.3f}, val_auc={val_auc:.3f}'
            tq.set_postfix_str(epoch_info)
    
            # å­¦ä¹ ç‡è°ƒåº¦
            if epoch < pre_epoch:
                scheduler1.step(train_recon)
            else:
                scheduler2.step(val_loss)
    
            # æ¨¡å‹ä¿å­˜å’Œæ—©åœ
            if epoch >= pre_epoch and val_auc > max_auc:
                max_auc = val_auc
                max_auc_epoch = epoch
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                state = {
                    'model': clf.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'epoch': epoch,
                    'val_auc': val_auc,
                    'train_history': train_history,
                    'val_history': val_history
                }
                
                torch.save(state, os.path.join(outdir, 'checkpoint', 'best_model.pth'))
                print(f"ä¿å­˜æœ€ä½³æ¨¡å‹ (Epoch {epoch+1}, AUC: {val_auc:.4f})")
            
            # æ—©åœæ£€æŸ¥
            if epoch >= pre_epoch and epoch - max_auc_epoch > 30:
                print(f"æ—©åœ: {30} epochs without improvement")
                break
    
    # ========== æµ‹è¯•é˜¶æ®µ ==========
    print("\nğŸš€ è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
    clf.eval()
    test_out = []
    with torch.no_grad():
        for seq, labels in test_loader:
            outputs = clf(seq.to(device))
            test_out.extend(torch.sigmoid(outputs['enhancer_score'].squeeze()).cpu().numpy())
                        
    test_scores = np.array(test_out)
    
    # è®¡ç®—æµ‹è¯•é›†æŒ‡æ ‡
    test_auc = metrics.roc_auc_score(test_labels, test_scores)
    
    print(f"Test AUC: {test_auc:.4f}")
    
    print(f"\nè®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯AUC: {max_auc:.4f} (Epoch {max_auc_epoch+1})")
    print(f"æœ€ç»ˆæµ‹è¯•AUC: {test_auc:.4f}")
    
    return max_auc, test_auc